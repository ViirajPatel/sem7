{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d3a1bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total days in the dataset 1452\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Dropout, Activation\n",
    "#country = 'India'\n",
    "df = pd.read_csv(\"INFYtday.csv\")\n",
    "# df = pd.read_csv('NSE-TATAGLOBAL11.csv')\n",
    "# df= pd.read_csv(\"NSE-TATA.csv\")\n",
    "df.index = df['Date']\n",
    "#creating dataframe\n",
    "data = df.sort_index(ascending=True, axis=0)\n",
    "new_data = pd.DataFrame(index=range(0, len(df)), columns=['Date', 'Close'])\n",
    "for i in range(0, len(data)):\n",
    "    new_data['Date'][i] = data['Date'][i]\n",
    "    new_data['Close'][i] = data['Close'][i]\n",
    "\n",
    "#setting index\n",
    "new_data.drop('Date', axis=1, inplace=True)\n",
    "new_data.index = pd.to_datetime(df[\"Date\"],format=\"%Y-%m-%d\")\n",
    "\n",
    "print(\"Total days in the dataset\", len(new_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1a547e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = math.ceil((len(new_data)*80)/100)\n",
    "\n",
    "train=new_data[:x]\n",
    "test = new_data[x:]\n",
    "\n",
    "##scale or normalize data as the data is too skewed\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaler.fit(train) \n",
    "\n",
    "train_scaled = scaler.transform(train)\n",
    "test_scaled = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27f102ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples in the original training data =  1162\n",
      "Total number of samples in the generated data =  1112\n"
     ]
    }
   ],
   "source": [
    "seq_size = 50  ## number of steps (lookback)\n",
    "n_features = 1 ## number of features. This dataset is univariate so it is 1\n",
    "train_generator = TimeseriesGenerator(train_scaled, train_scaled, length = seq_size, batch_size=1)\n",
    "print(\"Total number of samples in the original training data = \", len(train)) # 271\n",
    "print(\"Total number of samples in the generated data = \", len(train_generator)) # 264 with seq_size=7\n",
    "\n",
    "#Check data shape from generator\n",
    "x,y = train_generator[10] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2af166a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples in the original training data =  290\n",
      "Total number of samples in the generated data =  240\n"
     ]
    }
   ],
   "source": [
    "test_generator = TimeseriesGenerator(test_scaled, test_scaled, length=seq_size, batch_size=1)\n",
    "print(\"Total number of samples in the original training data = \", len(test)) # 14 as we're using last 14 days for test\n",
    "print(\"Total number of samples in the generated data = \", len(test_generator)) # 7\n",
    "#Check data shape from generator\n",
    "x,y = test_generator[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18995258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 50, 150)           91200     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                55040     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 150,465\n",
      "Trainable params: 150,465\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "10/10 [==============================] - 4s 195ms/step - loss: 0.0991 - val_loss: 0.0878\n",
      "Epoch 2/50\n",
      "10/10 [==============================] - 1s 157ms/step - loss: 0.0259 - val_loss: 0.0652\n",
      "Epoch 3/50\n",
      "10/10 [==============================] - 1s 161ms/step - loss: 0.0144 - val_loss: 0.0644\n",
      "Epoch 4/50\n",
      "10/10 [==============================] - 1s 147ms/step - loss: 0.0047 - val_loss: 3.8286\n",
      "Epoch 5/50\n",
      "10/10 [==============================] - 1s 148ms/step - loss: 0.0016 - val_loss: 129.5076\n",
      "Epoch 6/50\n",
      "10/10 [==============================] - 1s 149ms/step - loss: 0.0033 - val_loss: 41.3687\n",
      "Epoch 7/50\n",
      "10/10 [==============================] - 1s 148ms/step - loss: 0.0029 - val_loss: 1.4612\n",
      "Epoch 8/50\n",
      "10/10 [==============================] - 1s 148ms/step - loss: 0.0308 - val_loss: 361.2513\n",
      "Epoch 9/50\n",
      "10/10 [==============================] - 1s 146ms/step - loss: 0.0204 - val_loss: 0.4009\n",
      "Epoch 10/50\n",
      "10/10 [==============================] - 1s 149ms/step - loss: 0.0141 - val_loss: 0.0918\n",
      "Epoch 11/50\n",
      "10/10 [==============================] - 1s 149ms/step - loss: 0.0068 - val_loss: 0.0271\n",
      "Epoch 12/50\n",
      "10/10 [==============================] - 1s 147ms/step - loss: 0.0049 - val_loss: 0.0432\n",
      "Epoch 13/50\n",
      "10/10 [==============================] - 1s 148ms/step - loss: 0.0027 - val_loss: 0.1160\n",
      "Epoch 14/50\n",
      "10/10 [==============================] - 1s 147ms/step - loss: 0.0111 - val_loss: 0.1233\n",
      "Epoch 15/50\n",
      "10/10 [==============================] - 1s 148ms/step - loss: 0.0053 - val_loss: 0.0382\n",
      "Epoch 16/50\n",
      "10/10 [==============================] - 1s 148ms/step - loss: 8.7380e-04 - val_loss: 0.0247\n",
      "Epoch 17/50\n",
      "10/10 [==============================] - 1s 150ms/step - loss: 0.0025 - val_loss: 0.0864\n",
      "Epoch 18/50\n",
      "10/10 [==============================] - 1s 147ms/step - loss: 0.0023 - val_loss: 0.0428\n",
      "Epoch 19/50\n",
      "10/10 [==============================] - 1s 147ms/step - loss: 0.0137 - val_loss: 0.1541\n",
      "Epoch 20/50\n",
      "10/10 [==============================] - 1s 146ms/step - loss: 0.0097 - val_loss: 0.0693\n",
      "Epoch 21/50\n",
      "10/10 [==============================] - 1s 146ms/step - loss: 0.0014 - val_loss: 0.0181\n",
      "Epoch 22/50\n",
      "10/10 [==============================] - 1s 146ms/step - loss: 0.0019 - val_loss: 0.0185\n",
      "Epoch 23/50\n",
      "10/10 [==============================] - 1s 146ms/step - loss: 0.0013 - val_loss: 0.1075\n",
      "Epoch 24/50\n",
      "10/10 [==============================] - 1s 147ms/step - loss: 0.0099 - val_loss: 0.0114\n",
      "Epoch 25/50\n",
      "10/10 [==============================] - 1s 147ms/step - loss: 0.0029 - val_loss: 0.0342\n",
      "Epoch 26/50\n",
      "10/10 [==============================] - 2s 166ms/step - loss: 0.0024 - val_loss: 0.0105\n",
      "Epoch 27/50\n",
      "10/10 [==============================] - 1s 158ms/step - loss: 0.0075 - val_loss: 0.0105\n",
      "Epoch 28/50\n",
      "10/10 [==============================] - 2s 164ms/step - loss: 9.3310e-04 - val_loss: 0.0153\n",
      "Epoch 29/50\n",
      "10/10 [==============================] - 1s 148ms/step - loss: 0.0015 - val_loss: 0.0193\n",
      "Epoch 30/50\n",
      "10/10 [==============================] - 1s 146ms/step - loss: 0.0011 - val_loss: 0.0282\n",
      "Epoch 31/50\n",
      " 8/10 [=======================>......] - ETA: 0s - loss: 0.0082  "
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(150, activation='relu', return_sequences=True, input_shape=(seq_size, n_features)))\n",
    "model.add(LSTM(64, activation='relu'))\n",
    "model.add(Dense(64))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "model.summary()\n",
    "print('Train...')\n",
    "##########################\n",
    "\n",
    "history = model.fit_generator(train_generator, \n",
    "                              validation_data=test_generator, \n",
    "                              epochs=50, steps_per_epoch=10)\n",
    "\n",
    "\n",
    "#plot the training and validation accuracy and loss at each epoch\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "# plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "# plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "# plt.title('Training and validation loss')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Loss')\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3277b977",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from sklearn.metrics import mean_squared_error\n",
    "math.sqrt(mean_squared_error(train,train_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7787b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = [] #Empty list to populate later with predictions\n",
    "\n",
    "current_batch = train_scaled[-seq_size:] #Final data points in train \n",
    "current_batch = current_batch.reshape(1, seq_size, n_features) #Reshape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a2efba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e651c404",
   "metadata": {},
   "outputs": [],
   "source": [
    "future = 7 #Days\n",
    "for i in range(len(test) + future):\n",
    "    current_pred = model.predict(current_batch)[0]\n",
    "    prediction.append(current_pred)\n",
    "    current_batch = np.append(current_batch[:,1:,:],[[current_pred]],axis=1)\n",
    "\n",
    "### Inverse transform to before scaling so we get actual numbers\n",
    "rescaled_prediction = scaler.inverse_transform(prediction)\n",
    "\n",
    "time_series_array = test.index  #Get dates for test data\n",
    "\n",
    "#Add new dates for the forecast period\n",
    "for k in range(0, future):\n",
    "    time_series_array = time_series_array.append(time_series_array[-1:] + pd.DateOffset(1))\n",
    "\n",
    "#Create a dataframe to capture the forecast data\n",
    "df_forecast = pd.DataFrame(columns=[\"Close\",\"predicted\"], index=time_series_array)\n",
    "\n",
    "df_forecast.loc[:,\"predicted\"] = rescaled_prediction[:,0]\n",
    "df_forecast.loc[:,\"Close\"] = test[\"Close\"]\n",
    "\n",
    "#Plot\n",
    "#df_forecast.plot(title=\"Predictions for next 7 days\")\n",
    "#plt.show()\n",
    "df_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72420eda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a638ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b528bbd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9ef42f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fbdda7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f6fd1373de7469b4474cfd9f821dba682372631640c99a1fbd1e01b276364afd"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
